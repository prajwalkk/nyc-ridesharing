{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Final build for the Project\n",
    "\n",
    "### There are 3 main stages in this Project and one auxiliary stage which is run to set up the project. \n",
    "First, the `final_project.ipynb` is run to populate the PostGresDB and do some data cleaning. \n",
    "Later this file is run.  \n",
    "\n",
    "This file contains 3 phases:\n",
    "Data Generation -> Graph Building -> Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stage 1: Data Generation\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Danger:</b> Run Stage 0 before starting this\n",
    "</div>\n",
    "\n",
    "### Input: \n",
    "* Start time ``(2019-01-01 00:00:00)``\n",
    "* End time ``(2019-01-01 23:59:59)``\n",
    "* Poolsize ``(300, 420, 600)``\n",
    "    \n",
    "### Output: \n",
    "* CSV / Pandas DF that contains data for the next cycle\n",
    "    * The dataframe will contain these fields: \n",
    "         1. id\n",
    "         2. tpep_pickup_datetime\n",
    "         3. tpep_dropoff_datetime\n",
    "         4. passenger_count\n",
    "         5. trip_distance - acquired from OSRM\n",
    "         6. PULocationID\n",
    "         7. DOLocationID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import psycopg2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to DB\n",
    "conn_string = \"postgresql://nycrideshare:nycrideshare@127.0.0.1:5432/nyc_taxi\"\n",
    "nyc_database = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "# Get the adjacency matrix\n",
    "interzonal_dist = pd.read_csv(\"./data/interzonal.csv\")\n",
    "\n",
    "# Constants\n",
    "interval_dict = {\n",
    "    \"1hour\": \"\"\"1 HOURS\"\"\",\n",
    "    \"1day\": \"\"\"1 DAYS\"\"\",\n",
    "    \"1month\": \"\"\"1 MONTHS\"\"\"\n",
    "\n",
    "}\n",
    "pool_size = 600\n",
    "day_in_seconds = 24 * 3600\n",
    "max_passenger_count = 3\n",
    "\n",
    "# The start time representing the start of the analysis \n",
    "start_time = datetime(2019, 1,1, 00, 00,00)\n",
    "# Change the End time as required\n",
    "end_time = start_time + timedelta(minutes=60)\n",
    "\n",
    "# this flag represents if the pickup is at LGD or Dropoff\n",
    "lgd_flag = {\n",
    "    \"pickup\": 'PULocationID',\n",
    "    \"drop\": 'DOLocationID'\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(start_time, data_size_duration):\n",
    "    '''\n",
    "    This function aims to get a chunk of data for a month or day. Depending on the need.\n",
    "\n",
    "    [start_time]: datetime object\n",
    "    [data_size_duration]: one of the values specified in the interval_dict \n",
    "\n",
    "    '''\n",
    "    \n",
    "    time_string = start_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    query = \\\n",
    "        f\"\"\"select \n",
    "        id, \n",
    "        tpep_pickup_datetime, \n",
    "        tpep_dropoff_datetime, \n",
    "        passenger_count, \n",
    "        \"PULocationID\", \n",
    "        \"DOLocationID\"\n",
    "        from nyc_taxi_schema.get_cust_between_timestamps_lgd('{start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}', '{interval_dict[data_size_duration]}');\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Get the dataframe\n",
    "    df_temp = pd.read_sql_query(query, nyc_database)\n",
    "    # Add the distance to all the the rows\n",
    "    df_temp[\"Distance\"] = df_temp.apply(lambda row: interzonal_dist.iloc[row[\"PULocationID\"]-1, row[\"DOLocationID\"]-1], axis=1)\n",
    "\n",
    "    return df_temp.set_index(\"id\", drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for an hour of data\n",
    "data_size_duration = \"1hour\" # This is to get the initial dataframe\n",
    "df_generated = generate_data(start_time, data_size_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_iterator(ts_start, ts_end, delta_in_minutes, flag):\n",
    "    '''\n",
    "    This function is a generator function that returns filtered df rows between the paramerters passed\n",
    "\n",
    "    [ts_start]: datetime - start timestamp\n",
    "    [ts_end]: datetime - end timestamp\n",
    "    [delta_in_minutes]: int - The value specifies the timedelta for the poolsize\n",
    "    [flag]: string: it's value is either \"pickup\" or \"drop\"\n",
    "    '''\n",
    "    current = ts_start\n",
    "    delta = timedelta(minutes=delta_in_minutes)\n",
    "    while current < ts_end:\n",
    "        yield df_generated[\n",
    "            (df_generated['tpep_pickup_datetime'] >= current) & \n",
    "            (df_generated['tpep_pickup_datetime'] < current + delta) &\n",
    "            (df_generated[lgd_flag[flag]] == 138)]\n",
    "\n",
    "        current += delta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Graph Construction\n",
    "\n",
    "This stage is responsible to construct graphs using networX to model the relationships between passengers. \n",
    "The connected edges represent the rides that are merged. \n",
    "\n",
    "### Input Parameters: \n",
    "* Poolsize\n",
    "* Weight calculating functions as arguments\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the functions that calculate the weight\n",
    "\n",
    "def check_passenger_count(pool, indexA, indexB, max_passenger_count):\n",
    "    row1 = pool.loc[indexA, :]\n",
    "    row2 = pool.loc[indexB, :]\n",
    "    Pa, Pb = row1[\"passenger_count\"], row2[\"passenger_count\"]\n",
    "    return (max_passenger_count - (Pa + Pb) >= 0)\n",
    "\n",
    "def calc_distance_weight(row1, row2, flag):\n",
    "    Da = row1[\"Distance\"]\n",
    "    Db = row2[\"Distance\"]\n",
    "    column_name = \"DOLocationID\" if flag==\"pickup\" else \"PULocationID\"\n",
    "    Dab = interzonal_dist.iloc[\n",
    "        row1[column_name]-1, \n",
    "        row2[column_name]-1\n",
    "    ]\n",
    "    Dba = interzonal_dist.iloc[\n",
    "        row1[column_name]-1, \n",
    "        row2[column_name]-1\n",
    "    ]\n",
    "    Dmin = min(Da + Dab, Db + Dba)\n",
    "    savings = Da + Db - Dmin\n",
    "    return savings / (Da + Db)\n",
    "    \n",
    "def calc_time_weight(row1, row2, pool_size):\n",
    "    Ta = row1[\"tpep_pickup_datetime\"]\n",
    "    Tb = row2[\"tpep_pickup_datetime\"]\n",
    "    Tab = abs(Tb - Ta).seconds\n",
    "    return (pool_size - Tab) / pool_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_edge_weight(pool, indexA, indexB, distance_fn, time_fn, flag):\n",
    "    \n",
    "    row1 = pool.loc[indexA, :]\n",
    "    row2 = pool.loc[indexB, :]\n",
    "        \n",
    "    weight = distance_fn(row1, row2, flag) + time_fn(row1, row2, pool_size)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_filtered in enumerate(date_iterator(start_time, end_time, pool_size // 60, \"pickup\")):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    index_list = df_filtered.index.tolist()\n",
    "    G.add_nodes_from(index_list)\n",
    "    for indexA, indexB in combinations(index_list, 2):\n",
    "        \n",
    "        if not check_passenger_count(df_filtered, indexA, indexB, max_passenger_count):\n",
    "            continue\n",
    "        G.add_edge(indexA, indexB, weight=calc_edge_weight(df_filtered, indexA, indexB, calc_distance_weight, calc_time_weight, \"pickup\"))\n",
    "        \n",
    "    edge_set = nx.algorithms.matching.max_weight_matching(G)  \n",
    "    no_of_nodes = set(G.nodes)\n",
    "\n",
    "    pairs = set()\n",
    "\n",
    "    for i in edge_set:\n",
    "        tmp_set = set(i)\n",
    "        pairs = pairs.union(tmp_set)\n",
    "    missing_val = b.difference(pairs) \n",
    "    edge_set.union(missing_val)\n",
    "\n",
    "    show_viz(edge_set, df_filtered)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Visualization\n",
    "\n",
    "This stage is responsible to gather data from stage to for Visualization\n",
    "\n",
    "The idea for this phase is, Phase2 at each iteration calls this method. Parameters are TBD. \n",
    "When this function is called, the merged data and individual data is collated and stored as a DF/File. This can be used later to build graphs. \n",
    "\n",
    "### Input Parameters: \n",
    "* TBD\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_viz(edge_set, df_filtered):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0a0326998f42f45cdf4fa07683a4c17a5d1586f4c5cfcf7cdaf4ddb7918bb174f",
   "display_name": "Python 3.8.8 64-bit ('nyc_rideshare': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}