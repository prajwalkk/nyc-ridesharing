{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0a0326998f42f45cdf4fa07683a4c17a5d1586f4c5cfcf7cdaf4ddb7918bb174f",
   "display_name": "Python 3.8.8 64-bit ('nyc_rideshare': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# This is the Final build for the Project\n",
    "\n",
    "### There are 3 main stages in this Project and one auxiliary stage which is run to set up the project. \n",
    "First, the `final_project.ipynb` is run to populate the PostGresDB and do some data cleaning. \n",
    "Later this file is run.  \n",
    "\n",
    "This file contains 3 phases:\n",
    "Data Generation -> Graph Building -> Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Stage 1: Data Generation\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Danger:</b> Run Stage 0 before starting this\n",
    "</div>\n",
    "\n",
    "### Input: \n",
    "* Start time ``(2019-01-01 00:00:00)``\n",
    "* End time ``(2019-01-01 23:59:59)``\n",
    "* Poolsize ``(300, 420, 600)``\n",
    "    \n",
    "### Output: \n",
    "* CSV / Pandas DF that contains data for the next cycle\n",
    "    * The dataframe will contain these fields: \n",
    "         1. id\n",
    "         2. tpep_pickup_datetime\n",
    "         3. tpep_dropoff_datetime\n",
    "         4. passenger_count\n",
    "         5. trip_distance - acquired from OSRM\n",
    "         6. PULocationID\n",
    "         7. DOLocationID\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import psycopg2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Constants\n",
    "interval_dict = {\n",
    "    \"1hour\": \"\"\"1 HOURS\"\"\",\n",
    "    \"1day\": \"\"\"1 DAYS\"\"\",\n",
    "    \"1month\": \"\"\"1 MONTHS\"\"\"\n",
    "\n",
    "}\n",
    "pool_size = 300\n",
    "day_in_seconds = 24 * 3600\n",
    "\n",
    "\n",
    "# Connect to DB\n",
    "\n",
    "conn_string = \"postgresql://nycrideshare:nycrideshare@127.0.0.1:5432/nyc_taxi\"\n",
    "nyc_database = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(start_time, data_size_duration):\n",
    "    '''\n",
    "    This function aims to get a chunk of data for a month or day. Depending on the need.\n",
    "\n",
    "    [start_time]: datetime object\n",
    "    [data_size_duration]: one of the values specified in the interval_dict \n",
    "\n",
    "    '''\n",
    "    \n",
    "    time_string = start_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    query = \\\n",
    "        f\"\"\"select \n",
    "        id, \n",
    "        tpep_pickup_datetime, \n",
    "        tpep_dropoff_datetime, \n",
    "        passenger_count, \n",
    "        \"PULocationID\", \n",
    "        \"DOLocationID\"\n",
    "        from nyc_taxi_schema.get_cust_between_timestamps_lgd('{start_time.strftime(\"%Y-%m-%d %H:%M:%S\")}', '{interval_dict[data_size_duration]}');\"\"\"\n",
    "\n",
    "\n",
    "    # Get the adjacency matrix\n",
    "    interzonal_dist = pd.read_csv(\"./data/interzonal.csv\")\n",
    "    # Get the dataframe\n",
    "    df_temp = pd.read_sql_query(query, nyc_database)\n",
    "    # Add the distance to all the the rows\n",
    "    df_temp[\"Distance\"] = df_temp.apply(lambda row: interzonal_dist.iloc[row[\"PULocationID\"]-1, row[\"DOLocationID\"]-1], axis=1)\n",
    "\n",
    "    return df_temp.set_index(\"id\", drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_size_duration = \"1hour\" # This is to get the initial dataframe\n",
    "df_generated = generate_data(start_time, data_size_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_flag = {\n",
    "    \"pickup\": 'PULocationID',\n",
    "    \"drop\": 'DOLocationID'\n",
    "} # this flag represents if the pickup is at LGD or Dropoff\n",
    "\n",
    "def date_iterator(ts_start, ts_end, delta_in_minutes, flag):\n",
    "    current = ts_start\n",
    "    delta = timedelta(minutes=delta_in_minutes)\n",
    "    while current < ts_end:\n",
    "        yield df_generated[\n",
    "            (df_generated['tpep_pickup_datetime'] >= current) & \n",
    "            (df_generated['tpep_pickup_datetime'] < current + delta) &\n",
    "            (df_generated[lgd_flag[flag]] == 138)]\n",
    "\n",
    "        current += delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime(2019, 1,1, 00, 00,00)\n",
    "end_time = start_time + timedelta(minutes=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\nid                                                                  \n11866  2019-01-01 00:04:29   2019-01-01 00:26:02                1   \n\n       PULocationID  DOLocationID  Distance  \nid                                           \n11866           164           138   13397.4  \n     tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\nid                                                                 \n9596  2019-01-01 00:18:09   2019-01-01 00:41:11                1   \n\n      PULocationID  DOLocationID  Distance  \nid                                          \n9596           161           138   12556.8  \n    tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  PULocationID  \\\nid                                                                              \n261  2019-01-01 00:27:06   2019-01-01 00:51:51                1            68   \n\n     DOLocationID  Distance  \nid                           \n261           138   15110.6  \n      tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\nid                                                                  \n12778  2019-01-01 00:38:05   2019-01-01 01:14:38                1   \n\n       PULocationID  DOLocationID  Distance  \nid                                           \n12778            68           138   15110.6  \n      tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\nid                                                                  \n37707  2019-01-01 00:48:38   2019-01-01 01:27:28                2   \n\n       PULocationID  DOLocationID  Distance  \nid                                           \n37707           164           138   13397.4  \nEmpty DataFrame\nColumns: [tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, PULocationID, DOLocationID, Distance]\nIndex: []\n"
     ]
    }
   ],
   "source": [
    "# Just a demo for drop\n",
    "for df_filtered in date_iterator(start_time, end_time, 10, \"drop\"):\n",
    "    print(df_filtered.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "id\n5915   2019-01-01 00:02:51\n2455   2019-01-01 00:06:09\nName: tpep_pickup_datetime, dtype: datetime64[ns]\nid\n7296    2019-01-01 00:10:33\n11210   2019-01-01 00:13:38\nName: tpep_pickup_datetime, dtype: datetime64[ns]\nid\n5096   2019-01-01 00:21:07\n7697   2019-01-01 00:22:34\nName: tpep_pickup_datetime, dtype: datetime64[ns]\nid\n1336   2019-01-01 00:30:04\n1291   2019-01-01 00:30:45\nName: tpep_pickup_datetime, dtype: datetime64[ns]\nid\n7782   2019-01-01 00:40:18\n8064   2019-01-01 00:40:44\nName: tpep_pickup_datetime, dtype: datetime64[ns]\nid\n9487   2019-01-01 00:51:31\nName: tpep_pickup_datetime, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Just a demo for pickup\n",
    "for df_filtered in date_iterator(start_time, end_time, 10, \"pickup\"):\n",
    "    print(df_filtered['tpep_pickup_datetime'].head(2))"
   ]
  },
  {
   "source": [
    "## Stage 2: Graph Construction\n",
    "\n",
    "This stage is responsible to construct graphs using networX to model the relationships between passengers. \n",
    "The connected edges represent the rides that are merged. \n",
    "\n",
    "### Input Parameters: \n",
    "* Poolsize\n",
    "* Weight calculating functions as arguments\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stage 3: Visualization\n",
    "\n",
    "This stage is responsible to gather data from stage to for Visualization\n",
    "\n",
    "### Input Parameters: \n",
    "* TBD\n",
    "* TBD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}